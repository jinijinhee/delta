# delta

공유자동차운행일지데이터.csv는 row dataset이다.
먼저, 공유자동차_전처리코드_최종에 넣어서 1차전처리 코드를 돌린다.
이때 google api키가 필요한데, 저희 치타팀 api는 모두 소진하여 돌아가지 않음을 미리 알려드립니다...^^ 

그리고 1차 전처리가 끝난 최종 파일은 airport_add이다. 
이 코드를 가지고 네이버 크롤링 파일에 돌려 경로, 택시비, duration, 예상시간 등 가져왔다. 
이 크롤링이 끝난 파일이 airport_filtered 이다. 
그런데 저희는 주행의 시간대를 반영하기 위해 비행기 뜨는 시간대와 안뜨는 시간대로 나누어 두번을 돌렸습니다. 
그래서 시간대가 반영된 야간, 주간 파일 두개를 크롤링하여 생성해왔습니다. 그 파일이 df_야간_full, df_주간_full 입니다. 
주간, 야간 파일을 가지고 공유자동차_전처리코드_최종 파일의 섹션인 공유자동차 2차전처리 섹션을 전체 돌린다. 
이후 공유자동차_전처리코드_최종 파일의 섹션 공유자동차 2차전처리 이후 섹션들 1차 클러스터링(지역 및 비슷한 운행으로 군집)>>분류>>CNN 딥러닝 까지 돌릴 수 있다. 

파이스파크에서 분류 모델링을 하기 위해 타겟 라벨링을 숫자로 한 파일을 따로 저장하였다. (코드는 따로 남기지 않음. 헷갈림 유발해서.) 파일명은 df_modeling.csv이다. 

실시간을 구현하기 위해 소켓을 사용하는 방식을 구현하였는데 로컬에서 구현 가능하여 따로 파일로 저장함. >>소켓서버프로그래밍.ipynb

코랩에서 pyspark를 통해 모델링을 구현할 수 있도록 따로 파일을 저장함. >>pyspark_분류기

